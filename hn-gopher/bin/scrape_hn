#!/usr/bin/env python3

import os
import io
import json
import shutil
import logging
import argparse
import textwrap
import tempfile
from itertools import islice
from datetime import datetime
from logging.handlers import RotatingFileHandler

import requests
from lxml import html
from unidecode import unidecode

HN_URL = 'https://hacker-news.firebaseio.com/v0/'
# This API is nice because it can return all comments for a story with a
# single HTTP call
HN_SEARCH_URL = 'https://hn.algolia.com/api/v1/'


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--log-file')
    parser.add_argument('--data-dir', default='/var/gopher')
    parser.add_argument('--page-size', default=10)
    parser.add_argument('--page-count', default=10)
    parser.add_argument('--line-width', default=67)
    return parser.parse_args()


def setup_log(filename):
    logger = logging.getLogger('scrape_hn')
    logger.setLevel(logging.INFO)

    if filename:
        handler = RotatingFileHandler(
            filename=filename, maxBytes=5*1024*1024, backupCount=5)
    else:
        handler = logging.StreamHandler()

    fmt = logging.Formatter('%(asctime)s:%(levelname)s:%(filename)s:%(message)s')
    handler.setFormatter(fmt)
    logger.addHandler(handler)
    return logger


def sanitize(text):
    # 7-bit ASCII was good enough for 1991, so it's good enough for me
    # This package helps convert things like smart quotes to plain ascii
    return unidecode(text)


def humanize_timestamp(timestamp):
    timedelta = datetime.utcnow() - datetime.fromtimestamp(timestamp)

    seconds = int(timedelta.total_seconds())
    if seconds < 60:
        return 'moments ago'
    minutes = seconds // 60
    if minutes < 60:
        return '%d minutes ago' % minutes
    hours = minutes // 60
    if hours < 24:
        return '%d hours ago' % hours
    days = hours // 24
    if days < 30:
        return '%d days ago' % days
    months = days // 30.4
    if months < 12:
        return '%d months ago' % months
    years = months // 12
    return '%d years ago' % years


def story_generator(stories):
    for story_id in stories:
        resp = session.get('{0}item/{1}.json'.format(HN_URL, story_id))
        resp.raise_for_status()
        data = resp.json()
        if data['type'] in ('story', 'job'):
            yield data


def write_gophermap(directory, lines):
    os.makedirs(directory, exist_ok=True)
    gophermap = directory + '/gophermap'
    with io.open(gophermap, 'w+', encoding='ascii', errors='replace') as fp:
        fp.writelines('{0}\n'.format(line) for line in lines)


def dump_raw_story(directory, data):
    os.makedirs(directory, exist_ok=True)
    filename = directory + '/{0}.json'.format(data['id'])
    with io.open(filename, 'w', encoding='utf-8') as fp:
        json.dump(data, fp)


def update_story_file(story, page):

    resp = session.get('{0}/items/{1}'.format(HN_SEARCH_URL, story['id']))
    resp.raise_for_status()
    data = resp.json()

    out = [
        'i[Y] Hacker News - Item %s' % story['id'],
        '1(back)\t/live/p{0}'.format(page),
        'i ',
        'i' + text_repeater('_')]
    title = sanitize(story['title'])
    out.extend('i{0}'.format(l) for l in text_wrapper.wrap(title))

    if story.get('url'):
        url = sanitize(story['url'])
        out.append('h{0}\tURL:{0}'.format(url))

    info = 'i{0} points by {1}'.format(
        story['score'], sanitize(story['by']))
    out.append(info)

    out.extend([
        'i' + text_repeater('_'),
        '0Download Source\t/live/items/{0}/{0}.json'.format(data['id']),
        'i '])

    for child in data['children']:
        append_comment(child, out, level=0)

    item_dir = '{0}/items/{1}'.format(tmp_dir, story['id'])
    write_gophermap(item_dir, out)
    dump_raw_story(item_dir, data)


def append_comment(comment, out, level=0):
    indent = 'i' + min(level, 5) * 2 * ' '

    if 'author' in comment:
        info = indent + '{0} {1}'.format(
            sanitize(comment['author']),
            humanize_timestamp(comment['created_at_i']))
        out.append(info)

        # The text field contains HTML markup like <p> and <a>,
        # as well as HTML escaped characters.
        doc = html.fromstring(comment['text'])
        text = doc.text_content()

        wrapper = textwrap.TextWrapper(
            initial_indent=indent,
            subsequent_indent=indent,
            width=args.line_width + 1)  # +1 for the prepended "i"
        text_lines = wrapper.wrap(text)

        out.extend(text_lines)
    else:
        out.append(indent + '[deleted]')

    out.append('i ')

    for child in comment['children']:
        append_comment(child, out, level=level + 1)


def update_live_feed():

    _logger.info('Fetching list of top stories')
    resp = session.get('{0}{1}.json'.format(HN_URL, 'topstories'))
    resp.raise_for_status()
    data = resp.json()

    story_gen = story_generator(data)
    story_ids = []
    for page in range(1, args.page_count + 1):
        _logger.info('Building page %s', page)

        # Some gopher browsers (notably the iphone app) don't display an empty
        # comment line unless it contains at least one character, so add a space.
        out = [
            'i[Y] Hacker News (LIVE) - page {0} of {1}'.format(page, args.page_count),
            'i ',
            '1(home)\t/']

        if page != 1:
            out.extend(['1(previous page)\t/live/p{0}'.format(page - 1)])

        for story in islice(story_gen, args.page_size):
            story_ids.append(story['id'])
            out.append('i ')

            # Title and URL
            title = '{0}. {1}'.format(len(story_ids), sanitize(story['title']))
            lines = text_wrapper.wrap(title)
            if story.get('url'):
                url = sanitize(story['url'])
                out.extend(['h{0}\tURL:{1}'.format(l, url) for l in lines])
            else:
                url = '/items/{0}'.format(story['id'])
                out.extend(['1{0}\t{1}'.format(l, url) for l in lines])

            # Additional info
            if story['type'] == 'job':
                info = 'i{0} points {1}'.format(
                    story['score'],
                    humanize_timestamp(story['time']))
                out.extend([info])
            else:
                info = 'i{0} points by {1} {2} | {3} comments'.format(
                    story['score'],
                    sanitize(story['by']),
                    humanize_timestamp(story['time']),
                    story['descendants'])
                comments = '1View comments\t/live/items/{0}'.format(story['id'])
                out.extend([info, comments])

            # Update the story's comment file
            update_story_file(story, page)

        if page != args.page_count:
            out.extend(['i ', '1(next page)\t/live/p{0}'.format(page + 1)])

        page_dir = '{0}/p{1}'.format(tmp_dir, page)
        write_gophermap(page_dir, out)


##############################################################################
# Main
##############################################################################
args = parse_args()

text_wrapper = textwrap.TextWrapper(args.line_width)
text_repeater = lambda char: char * args.line_width

session = requests.session()
_logger = setup_log(args.log_file)

# Stage everything in a temporary directory
with tempfile.TemporaryDirectory() as tmp_dir:
    _logger.info('Setting up staging directory %s', tmp_dir)
    os.chmod(tmp_dir, 0o755)

    update_live_feed()

    live_dir = args.data_dir + '/live'

    _logger.info('Clearing existing stories')
    shutil.rmtree(live_dir, ignore_errors=True)

    _logger.info('Copying staging directory to %s', live_dir)
    shutil.copytree(tmp_dir, live_dir)

_logger.info('Finished!')
